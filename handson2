import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import make_scorer, mean_squared_error
import numpy as np

price_reg=pd.read_csv('C:/Users/shirs/Desktop/NY-House-Dataset.csv')
price_reg.dropna (inplace=True)
target = 'PRICE'

# Define categorical and numerical columns
categorical_cols =['BROKERTITLE', 'TYPE', 'ADDRESS', 'STATE', 'MAIN_ADDRESS', 'ADMINISTRATIVE_AREA_LEVEL_2', 'LOCALITY', 'SUBLOCALITY', 'STREET_NAME', 'LONG_NAME', 'FORMATTED_ADDRESS']
numerical_cols =['BEDS', 'BATH', 'PROPERTYSQFT', 'LATITUDE', 'LONGITUDE']
# One-hot encode categorical features
encoder = OneHotEncoder(drop="first", sparse_output=False)
encoded_data = encoder.fit_transform(price_reg[categorical_cols])
# Create a Dataframe from encoded data
encoded_columns = encoder.get_feature_names_out (categorical_cols)
df_encoded = pd.DataFrame(encoded_data, columns=encoded_columns, index=price_reg.index)
# Combine numerical data and encoded categorical data
x = pd.concat([price_reg[numerical_cols], df_encoded], axis=1)
y = price_reg[target]
# Scale the features for better performance of regression models
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.3, random_state=42)

# Initialize and train the Linear Regression model
linear_model = LinearRegression()
linear_model.fit(x_train, y_train)

# Predict on the testing set
y_pred_linear = linear_model.predict(x_test)

# Evaluate the Linear Regression model
print("Linear Regression Mean Squared Error:", mean_squared_error(y_test, y_pred_linear))
print("Linear Regression R^2 Score:", r2_score (y_test, y_pred_linear))

# Initialize and train the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(x_train, y_train)

# Predict on the testing set
y_pred_rf = rf_model.predict(x_test)

# Evaluate the Random Forest Regressor
print("Random Forest Regressor Mean Squared Error:", mean_squared_error(y_test, y_pred_rf))
print("Random Forest Regressor R^2 Score:", r2_score (y_test, y_pred_rf))

# Assuming x and y are your features and target variable
regressor = RandomForestRegressor(
    n_estimators=300,
    max_depth=5,
    min_samples_split=5,
    min_samples_leaf=4,
    max_features='sqrt',
    random_state=42
)

# Set up k-fold cross-validation (5 folds)
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform cross-validation and calculate MSE for each fold
scores = cross_val_score(
    regressor, x, y, cv=kf, scoring=make_scorer(mean_squared_error)
)

# Calculate the mean and standard deviation of the MSE scores
mean_mse = np.mean(scores)
std_mse = np.std(scores)
print(f"Mean MSE from cross-validation: {mean_mse:.2f}")

# Fit model to entire dataset for R^2 score demonstration
regressor.fit(x, y)
y_pred = regressor.predict(x)
r2 = r2_score(y, y_pred)
print(f"R^2 Score: {r2:.2f}")



# Assuming x_train, x_test, y_train, y_test are already defined as your train/test splits

# L2 Regularization: Ridge Regression
ridge = Ridge(alpha=1.0)  # alpha controls the regularization strength; higher alpha means more regularization
ridge.fit(x_train, y_train)
ridge_predictions = ridge.predict(x_test)
print("Ridge MSE:", mean_squared_error(y_test, ridge_predictions))
print("Ridge R^2 Score:", r2_score(y_test, ridge_predictions))

# L1 Regularization: Lasso Regression
lasso = Lasso(alpha=0.1)  # alpha is the regularization parameter; higher alpha means more regularization
lasso.fit(x_train, y_train)
lasso_predictions = lasso.predict(x_test)
print("Lasso MSE:", mean_squared_error(y_test, lasso_predictions))
print("Lasso R^2 Score:", r2_score(y_test, lasso_predictions))

# L1 and L2 Regularization: Elastic Net
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio balances between L1 and L2 (0 = pure L2, 1 = pure L1)
elastic_net.fit(x_train, y_train)
elastic_net_predictions = elastic_net.predict(x_test)
print("Elastic Net MSE:", mean_squared_error(y_test, elastic_net_predictions))
print("Elastic Net R^2 Score:", r2_score(y_test, elastic_net_predictions))
